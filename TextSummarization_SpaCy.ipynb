{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization SpaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlPEjmbiSLI3FODPjE8U8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saied-ahemd/text_summarization/blob/main/TextSummarization_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRDRMn6ePg8L"
      },
      "source": [
        "import spacy\r\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\r\n",
        "from string import punctuation as pun\r\n",
        "from heapq import nlargest"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Kw_n0OsT3_B"
      },
      "source": [
        "text = \"\"\"There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\r\n",
        "\r\n",
        "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\r\n",
        "\r\n",
        "Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[5] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\r\n",
        "\r\n",
        "At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\"\"\"\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "doc = nlp(text)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-83QRRHYqg0"
      },
      "source": [
        "# now let's make tokens from this text \r\n",
        "tokens = [token.text for token in doc]\r\n",
        "\r\n",
        "# now let's add to the pun the \\n to remove from the text \r\n",
        "pun = pun + '\\n'\r\n",
        "\r\n",
        "# now let's get the frq of the word in the text \r\n",
        "word_freq = {}\r\n",
        "for word in doc:\r\n",
        "  # now we remove the stop word from our text \r\n",
        "  if word.text.lower() not in stopwords:\r\n",
        "    if word.text.lower() not in pun:\r\n",
        "      if word.text not in word_freq.keys():\r\n",
        "        # this mean it's the first time we add this to the dic\r\n",
        "        word_freq[word.text] = 1\r\n",
        "      else:\r\n",
        "        word_freq[word.text] += 1\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAeqJpU9cGWV"
      },
      "source": [
        "# now we will get the max numner of freq word and divied all the words by this nuber so that the max number will be one\r\n",
        "max_frq = max(word_freq.values())\r\n",
        "\r\n",
        "for word in word_freq.keys():\r\n",
        "  word_freq[word] = word_freq[word] / max_frq\r\n",
        "  \r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SsgvfbqdGvj"
      },
      "source": [
        "# now we will get the sentence \r\n",
        "sentences_tokens = [sen for sen in doc.sents]\r\n",
        "\r\n",
        "# now we will get the sentece score just like the word \r\n",
        "sentece_score = {}\r\n",
        "\r\n",
        "for sent in sentences_tokens:\r\n",
        "  for word in sent:\r\n",
        "    if word.text.lower() in word_freq.keys():\r\n",
        "      if sent not in sentece_score.keys():\r\n",
        "        sentece_score[sent] = word_freq[word.text.lower()]\r\n",
        "      else:\r\n",
        "        sentece_score[sent] += word_freq[word.text.lower()]\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSGgpD0pgOst"
      },
      "source": [
        "# now we will get the 30% of the sents \r\n",
        "selected_len  = int(len(sentences_tokens) * 0.3)\r\n",
        "\r\n",
        "# now this give us the most frq 5 sents\r\n",
        "summray = nlargest(selected_len,sentece_score,sentece_score.get)\r\n",
        "\r\n",
        "# now we will combine the sents together\r\n",
        "final_sum = [word.text for word in summray]\r\n",
        "summray = ' '.join(final_sum)"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}